#!/usr/bin/env python

import sys
import urllib
import os
import subprocess



# (c) 2014 Everaldo Aguiar & Reid Johnson
#
# Modified from:
# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)
#
# Functions to compute and extract association rules from a given frequent itemset 
# generated by the Apriori algorithm.
#
# The Apriori algorithm is defined by Agrawal and Srikant in:
# Fast algorithms for mining association rules
# Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994


#Modified by Haile Misgna


def load_dataset():
     dataset = []
     flist = ['transaction-r-00000','transaction-r-00001']
     #input comes from standard input
     for file in flist:
        with open ('transactions/' + file ) as f:
                for line in f:
                        line = line.replace('[','').replace(']','').replace("u'","").replace("'",'').replace("\n",'')
                        all = line.split(', ')
                        dataset.append(all)
     return dataset


def create_candidates(dataset, prnt=False):
    '''Creates a list of candidate 1-itemsets from a list of transactions.
    Args:
      dataset (list): The dataset (a list of transactions) from which to generate 
        candidate itemsets.

    Returns:
      frozenset mapping of c1: The list of candidate itemsets (c1) passed as a 
        frozenset (a set that is immutable and hashable).

    '''
    c1 = [] # list of all items in the database of transactions
    for transaction in dataset:
        for item in transaction:
            if not [item] in c1:
                c1.append([item])
    c1.sort()

    # Print a list of all the candidate items
    if prnt:
        print "" \
            + "{" \
            + "".join(str(i[0]) + ", " for i in iter(c1)).rstrip(', ') \
            + "}"

    # map c1 to a frozenset because it will be the key of a dictionary
    return map(frozenset, c1)

def support_prune(dataset, candidates, min_support, prnt=False):
    '''Returns all candidate itemsets that meet a minimum support threshold.

    By the apriori principle, if an itemset is frequent, then all of its subsets must
    also be frequent. As a result, we can perform support-based pruning to systematically
    control the exponential growth of candidate itemsets. Thus, itemsets that do not 
    meet the minimum support level are pruned from the input list of itemsets (dataset).

    Args:
 dataset (list): The dataset (a list of transactions) from which to generate 
        candidate itemsets.
      candidates (frozenset): The list of candidate itemsets.
      min_support (float): The minimum support threshold.

    Returns:
      retlist: The list of frequent itemsets.
      support_data: A dict of the support data for all candidate itemsets.

    '''
    sscnt = {} # set for support counts
    for tid in dataset:
        for can in candidates:
            if can.issubset(tid):
                sscnt.setdefault(can, 0)
                sscnt[can] += 1

    num_items = float(len(dataset)) # total number of transactions in the dataset
    retlist = [] # array for unpruned itemsets
    support_data = {} # set for support data for corresponding itemsets
    for key in sscnt:
        # calculate the support of itemset key
        support = sscnt[key] / num_items
        if support >= min_support:
            retlist.insert(0, key)
        support_data[key] = support

    # Print a list of the pruned itemsets
    if prnt:
        for kset in retlist:
            for item in kset:
                print "{" + str(item) + "}"
        print
        for key in sscnt:
            print "" \
                + "{" \
                + "".join([str(i) + ", " for i in iter(key)]).rstrip(', ') \
                + "}" \
                + ":  sup = " + str(support_data[key])

    return retlist, support_data

def apriori_gen(freq_sets, k):
    '''Generates candidate itemsets (via the F_k-1 x F_k-1 method).

    This operation generates new candidate k-itemsets based on the frequent (k-1)-itemsets
    found in the previous iteration. The candidate generation procedure merges a pair of
    frequent (k-1)-itemsets only if their first k-2 items are identical.

    Args:
      freq_sets (list): The list of frequent (k-1)-itemsets.
      k (int): The cardinality of the current itemsets begin evaluated.

    Returns:
      retlist: The list of merged frequent itemsets.

    '''
    retList = [] # list of merged frequent itemsets
    lenLk = len(freq_sets) # number of frequent itemsets
    for i in range(lenLk):
        for j in range(i+1, lenLk):
            F1 = list(freq_sets[i])[:k-2] # first k-2 items of freq_sets[i]
            F2 = list(freq_sets[j])[:k-2]# first k-2 items of freq_sets[j]
            F1.sort()
            F2.sort()
            if F1 == F2: # if the first k-2 items are identical
                # merge the frequent itemsets
                retList.append(freq_sets[i] | freq_sets[j])

    return retList
def apriori(dataset, min_support=0.5):
    '''Generates a list of candidate itemsets.
    
    The Apriori algorithm will iteratively generate new candidate k-itemsets using the 
    frequent (k-1)-itemsets found in the previous iteration.

    Args:
      dataset (list): The dataset (a list of transactions) from which to generate 
        candidate itemsets.
      min_support (float): The minimum support threshold. Defaults to 0.5.

    Returns:
      F: The list of frequent itemsets.
      support_data: A dict of the support data for all candidate itemsets.

    '''
    print "Apriori is fired"
    print "---->candidate sets are in generation"
    C1 = create_candidates(dataset)
    D = map(set, dataset)
    print "----->support pruning in progress"
    F1, support_data = support_prune(D, C1, min_support) # prune candidate 1-itemsets
    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets
    k = 2 # the itemset cardinality
    while (len(F[k - 2]) > 0):
        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets
        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets
        support_data.update(supK) # update the support counts to reflect pruning
        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets
        k += 1

    # Print a list of all the frequent itemsets
    for kset in F:
        for item in kset:
            print "" \
                + "{" \
                + "".join(str(i) + ", " for i in iter(item)).rstrip(', ') \
                + "}" \
                + ":  sup = " + str(round(support_data[item], 3))
 return F, support_data

def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.7):
    '''Generates a set of candidate rules.

    Args:
      freq_set (frozenset): The complete list of frequent itemsets.
      H (list): A list of frequent itemsets (of a particular length).
      support_data (dict): The support data for all candidate itemsets.
      rules (list): A potentially incomplete set of candidate rules above the minimum 
        confidence threshold.
      min_confidence (float): The minimum confidence threshold. Defaults to 0.7.

    '''
    m = len(H[0])
    if (len(freq_set) > (m+1)):
        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets
        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence)
        if len(Hmp1) > 1:
            # If there are candidate rules above the minimum confidence threshold, recurse 
            # on the list of these candidate rules.
            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence)

def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.7):
    '''Evaluates the generated rules.

    One measurement for quantifying the goodness of association rules is confidence.
    The confidence for a rule 'P implies H' (P -> H) is defined as the support for P and H
    divided by the support for P (support (P|H) / support(P)), where the | symbol denotes
    the set union (thus P|H means all the items in set P or in set H).

    To calculate the confidence, we iterate through the frequent itemsets and associated
    support data. For each frequent itemset, we divide the support of the itemset by the
    support of the antecedent (left-hand-side of the rule).

    Args:
      freq_set (frozenset): The complete list of frequent itemsets.
      H (list): A frequent itemset.
      min_support (float): The minimum support threshold.
Returns:
      rules: The list of candidate rules above the minimum confidence threshold.

    '''
    rules = []
    for i in range(1, len(F)):
        for freq_set in F[i]:
            H1 = [frozenset([item]) for item in freq_set]

            if (i > 1):
                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence)
            else:
                calc_confidence(freq_set, H1, support_data, rules, min_confidence)

    return rules

if __name__ == "__main__":
        #load_dataset()
        apriori(load_dataset(), min_support=0.1)
        #generate_rules(apriori(load_dataset(), 0.1),0.1)



